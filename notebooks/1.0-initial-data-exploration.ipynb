{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nova Financial Solutions - Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Functions\n",
    "In this section, import all necessary modules from the `shared` module to load and clean the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.dataset_loader import load_raw_dataset\n",
    "from shared.data_cleaning import (\n",
    "     date_to_datetime,\n",
    "     handle_missing_values, \n",
    "     standardize_text,\n",
    "     handle_duplicates,\n",
    "     save_processed_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Raw Dataset\n",
    "First, we need to load the raw dataset using the `load_raw_dataset` function. This function loads the data from a CSV file and returns a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_raw_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert date to datetime\n",
    "This function takes the raw dataset as input and converts any date columns to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = date_to_datetime(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values\n",
    "The `handle_missing_values` function handles missing values in the dataset. It Drop rows with empty headline, url, date, stock columns and Replace empty row publisher column with Unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the handle_missing_values function on the processed dataset\n",
    "processed_data = handle_missing_values(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize text\n",
    "The `standardize_text` function is used to standardize text data in the dataset. It performs tasks such as converting text to lowercase, removing special characters or punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the standardize_text function on the processed dataset\n",
    "processed_data = standardize_text(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle duplicates\n",
    "The `handle_duplicates` function deals with duplicate records in the dataset. It Check for duplicate rows based on the url column & Keep only the row with the latest date compared to all duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the handle_duplicates function on the processed dataset\n",
    "processed_data = handle_duplicates(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed dataset\n",
    "The `save_processed_dataset` function allows to save the cleaned and preprocessed dataset to a CSV file. Create the output folder if it doesn't exist and Save the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed dataset\n",
    "output_folder = os.path.join('..', 'data', 'processed') \n",
    "save_processed_dataset(processed_data, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis using the Cleaned and Processed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Functions\n",
    "In this section, import all necessary functions from the `shared` module for loading the cleaned dataset and perfom analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.dataset_loader import load_cleaned_dataset\n",
    "from shared.descriptive_analysis import (\n",
    "     get_text_length_stats,\n",
    "     count_articles_per_publisher,\n",
    "     identify_unique_domains\n",
    ")\n",
    "from shared.inferential_analysis import (\n",
    "     analyze_publication_dates,\n",
    "     perform_sentiment_analysis\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cleaned Dataset\n",
    "First, we need to load the cleaned dataset using the `load_cleaned_dataset` function. This function loads the data from a CSV file and returns a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = load_cleaned_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistical Analysis\n",
    "In this section, we define functions that perform Descriptive statistical data analysis tasks on a iven dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text length stats\n",
    "\n",
    "This function calculates various statistics related to the length of the headlines in the given dataset. It takes the dataset as input and returns a dictionary with the following statistics:\n",
    "\n",
    "- `mean`: The mean length of the headlines.\n",
    "- `median`: The median length of the headlines.\n",
    "- `minimum`: The minimum length of the headlines.\n",
    "- `maximum`: The maximum length of the headlines.\n",
    "- `standard_deviation`: The standard deviation of the lengths of the headlines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "News Stats:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 73.12051490484095,\n",
       " 'median': 64.0,\n",
       " 'minimum': 3,\n",
       " 'maximum': 512,\n",
       " 'standard_deviation': 40.73530993195065}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "stats = get_text_length_stats(cleaned_data)\n",
    "print(\"\\nNews Stats:\")\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of articles per publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News frequency by day of the week:\n",
      "publisher\n",
      "Paul Quintaro                      228373\n",
      "Lisa Levin                         186979\n",
      "Benzinga Newsdesk                  150484\n",
      "Charles Gross                       96732\n",
      "Monica Gerson                       82380\n",
      "                                    ...  \n",
      "Shazir Mucklai - Imperium Group         1\n",
      "Laura Jennings                          1\n",
      "Eric Martin                             1\n",
      "Jose Rodrigo                            1\n",
      "Jeremie Capron                          1\n",
      "Name: count, Length: 1034, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "publisher_counts = count_articles_per_publisher(cleaned_data)\n",
    "# Print the results\n",
    "print(\"News frequency by day of the week:\")\n",
    "print(publisher_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify unique domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Domains:\n",
      "['' 'benzinga' 'gmail' 'andyswan' 'investdiva' 'eosdetroit' 'tothetick'\n",
      " 'forextraininggroup' 'stockmetrix']\n"
     ]
    }
   ],
   "source": [
    "unique_domains = identify_unique_domains(cleaned_data)\n",
    "# Print the results\n",
    "print(\"Unique Domains:\")\n",
    "print(unique_domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferential Statistical Data Analysis\n",
    "In this section, we define functions that perform Inferential statistical data analysis tasks on the cleaned dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the publication dates\n",
    "Analyze the publication dates in the dataset to identify trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_publication_dates(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform sentiment analysis on headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_sentiment_analysis(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
